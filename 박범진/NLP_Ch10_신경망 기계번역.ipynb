{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "신경망 기계번역\n",
    "================\n",
    "\n",
    "Category\n",
    "\n",
    "\n",
    "-  Encoder\n",
    "-  Packed Padded Sequence\n",
    "-  Generator\n",
    "-  Attention\n",
    "-  BeamSearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Encoder\n",
    "층을 통과하면 결과적으로 문장 임베딩 벡터를 생성 \n",
    "\n",
    "양방향 LSTM 으로 하려면 Bidirectional  = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import *\n",
    "import torch.nn as nn\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, word_vec_dim, hidden_size, n_layers=4, dropout_p = .2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.LSTM(wrod_vec_dim,\n",
    "                          int(hidden_size/2),\n",
    "                          num_layers=n_layers,\n",
    "                           dropout = dropout_p,\n",
    "                           bidirectional=True,\n",
    "                           batch_first = True\n",
    "                          )\n",
    "        \n",
    "    def forward(self, emb):\n",
    "        \n",
    "        if ininstance(emb, tuple):\n",
    "            x,lengths = emb\n",
    "            x = pack(x,lengths.tolist(), batch_first = True)\n",
    "            \n",
    "        else:\n",
    "            x = emb\n",
    "            \n",
    "        y,h = self.rnn(x)\n",
    "        \n",
    "        if isinstance(emb, tuple):\n",
    "            y,_ = unpack(y,batch_first=True)\n",
    "        \n",
    "        return y,h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Packed Padded Sequence\n",
    "\n",
    "기존의 샘플별 미니배치를 time-step별로 표현해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([1, 3, 2, 4, 3]), batch_sizes=tensor([2, 2, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import *\n",
    "a = [torch.tensor([1,2,3]), torch.tensor([3,4])]\n",
    "b = torch.nn.utils.rnn.pad_sequence(a, batch_first = True)\n",
    "tensor([[1,2,3],\n",
    "        [3,4,0]])\n",
    "torch.nn.utils.rnn.pack_padded_sequence(b, batch_first = True, lengths=[3,2])\n",
    "PackedSequence(data = tensor([1,3,2,4,3]), batch_sizes = tensor([2,2,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        supre(Generator, self).__init__()\n",
    "        \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.softmax(self.output(x))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # logsoftmax 함수를 이용해서 로그 확률을 구한다.\n",
    "    def _get_loss(self, y_hat, y, crit=None):\n",
    "        crit = self.crit if crit is None else crit\n",
    "        loss = crit(y_hat.contiguous().view(-1, y_hat.size(-1)),\n",
    "                   y.contiguous().view(-1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Attention\n",
    "\n",
    "파이썬 딕셔러니를 이용한 key  - value를 활용한다. \n",
    "전체적인 스코어를 총합하는 과정이 필요하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how_similar 는 코사인 유사도를 반환\n",
    "# Query 는 벡터 임베팅 되었다고 가정\n",
    "\n",
    "\n",
    "def key_value_func(query):\n",
    "    weights = []\n",
    "    \n",
    "    for key in dic.keys():\n",
    "        weights += [how_similar(key, query)]\n",
    "    \n",
    "    weights = softmax(weights)\n",
    "    answer = 0\n",
    "    \n",
    "    for weight, value in zip(weights, dic.values()):\n",
    "        answer += weight * value\n",
    "    \n",
    "    return answer\n",
    "# encoder에서 time-step별 출력을 키와 벨류로 삼고, \n",
    "# 현재 time-step의 디코더 출력을 쿼리로 삼아 어텐션을 계산한다. \n",
    "# 가중합\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention,self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    def forward(self, h_src, h_t_tgt, mask=None):\n",
    "        query = self.linear(h_t_tgt.squeeze(1)).unsqueecze(-1)\n",
    "        \n",
    "        weight = torch.bmm(h_src, query).squeeze(-1)\n",
    "        \n",
    "        if mask is not None:\n",
    "            weight.masked_fill_(mask, -float('inf'))\n",
    "        weight = self.softmax(weight)\n",
    "        \n",
    "        context_vector = torch.bmm(weight.unsqueeze(1), h_src)\n",
    "        \n",
    "        return context_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate_mask\n",
    "\n",
    "문장마다 길이가 다르므로 가장 길이가 긴 것을 기준으로 필요없는 부분에 대하여 1의 값을 채워준다. \n",
    " 이 경우, 어텐션 가중치를 0으로 만들어 더해도 상관없는 값으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(self, x, length):\n",
    "    mask = []\n",
    "    \n",
    "    max_length = max(length)\n",
    "    for l in length:\n",
    "        if max_length -1 >0:\n",
    "            mask += [torch.cat([x.new_ones(1,l).zero_(),\n",
    "                               x.new_ones(1, (max_length -l))], dim = -1)]\n",
    "        else:mask += [x.new_ones(1,l).zero_()]\n",
    "    \n",
    "    mask = torch.cat(mask, dim=0).byte()\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Class\n",
    "\n",
    "앞에서 정의한 인코더, 디코더, 생성자와 어텐션 클래스를 활용한 전체 seq2seq 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, word_vec_dim, hidden_size, output_size, n_layers=4, dropout_p=.2):\n",
    "        self.input_size = input_size\n",
    "        self.word_vec_dim = word_vec_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.emb_src = nn.Embedding(input_size, word_vec_dim)\n",
    "        self.emb_dec = nn.Embedding(output_size, word_vec_dim)\n",
    "        \n",
    "        self.encoder = Encoder(word_vec_dim,\n",
    "                              hidden_size,\n",
    "                              n_layers=n_layers,\n",
    "                              dropout_p=dropout_p)\n",
    "        self.decoder = Decoder(word_vec_dim,\n",
    "                              hidden_size,\n",
    "                              n_layers=n_layers,\n",
    "                              dropout_p=dropout_p)\n",
    "        self.attn = Attention(hidden_size)\n",
    "        \n",
    "        self.concat = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.generator = Generator(hddien_size, output_size)\n",
    "        \n",
    "    def merge_encoder_hiddens(self, encoder_hiddens):\n",
    "        new_hiddens = []\n",
    "        new_cells =[]\n",
    "        \n",
    "        hiddens, cells = encoder_hiddens\n",
    "        \n",
    "        for i in range(0, hidden.size(0),2):\n",
    "            new_hiddens += [torch.cat([hiddens[i], hiddens[h+1]], dim = -1)]\n",
    "            new_cells += [torch.cat([cells[i], cells[i+1]], dim = -1)]\n",
    "        \n",
    "        new_hiddens, new_cells = torch.stack(new_hiddens),torch.stack(new_cells)\n",
    "        \n",
    "        return (new_hiddens, new_cells)\n",
    "        #반복문을 순차적으로하기 보다 병렬적인 작업으로 변경해줘야 한다. \n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        batch_size = tgt.size(0)\n",
    "        \n",
    "        mask = None\n",
    "        x_length = None\n",
    "        if isinstance(src, tuple):\n",
    "            x, x_lenght = src\n",
    "            mask = self.generate_mask(x, x_length)\n",
    "            \n",
    "        else:\n",
    "            x = src\n",
    "            if isinstance(tgt, tuple):\n",
    "                tgt =tgt[0]\n",
    "            \n",
    "        emb_src = self.emb_src(x)\n",
    "        h_src, h_0_tgt = self.encoder((emb_src, x_length))\n",
    "        h_0_tgt, c_0,_tgt = h_0_tgt\n",
    "        h_0_tgt = h_0_tgt.transpose(0,1).contiguous().view(bathc_size,-1,\n",
    "                                                           self.hidden_size).tranpose(0,1).contiguous()\n",
    "        h_0_tgt = (h_0_tgt, c_0_tgt)\n",
    "        emb_tgt = self.emb_dec(tgt)\n",
    "        h_tilde = []\n",
    "        h_t_tilde = None\n",
    "        decoder_hidden = h_0_tgt\n",
    "        # time -steop의 끝까지 decoder를 Run\n",
    "        for i in range(tgt.size(1)):\n",
    "            emb_t = emb_tgt[:,t,:].unsqueeze(1)\n",
    "            decoder_output, decoder_hidden = self.decoder(emb_t, h_t_tilde, decoder_hidden)\n",
    "            context_vector = self.attn(h_src, decoder_output,mask)\n",
    "            h_t_tilde = self.tanh(Self.concat(torch.cat([decoder_output,\n",
    "                                                        context_vector],\n",
    "                                                        dim=-1)))\n",
    "            h_tilde += [h_t_tilde]\n",
    "        \n",
    "        h_tilde = torch.cat(h_tilde, dim=1)\n",
    "        y_hat = self.generator(h_tilde)\n",
    "        \n",
    "        return y_hat\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching forcing : 다음 input 에 그 전 단계의 결과값을 넣는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
