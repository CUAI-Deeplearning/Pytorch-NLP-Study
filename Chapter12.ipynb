{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter12.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEFOzSIfgRP/fM2aUEtNfp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CUAI-Deeplearning/Pytorch-NLP-Study/blob/stats/Chapter12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOu6-wVWtIRT",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 12. 강화학습을 활용한 자연어 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oIrre-MuY9L",
        "colab_type": "text"
      },
      "source": [
        "## 12.6 강화학습을 활용한 비지도학습\n",
        "\n",
        "* 지도학습 : 높은 정확도, 높은 비용과 시간(레이블 데이터 필요)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo1H6WnjtaRN",
        "colab_type": "text"
      },
      "source": [
        "* 비지도학습 : 데이터 확보 시 비용과 시간 훨씬 절감 가능, 효율은 떨어진다.\n",
        "* 병렬 코퍼스에 비해 쉬운 단일 언어 코퍼스가 대안이 된다.\n",
        "* 소량의 병렬 코퍼스 + 다량의 단일 언어 코퍼스 = 더 나은 성능 확보 가능\n",
        "* back translation, copied translation에도 NMT 성능 고도화 가능하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cxvBRy6uiyS",
        "colab_type": "text"
      },
      "source": [
        "### 12.6.1 비지도학습을 활용한 NMT\n",
        "* Cross-lingual Language Model Pretraining\n",
        "  * 오직 단일 언어 코퍼스만을 사용해 번역기 제작\n",
        "  * 언어에 상관 없이 같은 의미의 문장일 경우 인코더가 같은 값으로 임베딩하도록 훈련한다.\n",
        "  * 생성적 적대 신경망(GAN) 도입\n",
        "  * 인코더의 출력값이 연속적인 값이므로 GAN 적용 가능하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db7lOoCQwDlH",
        "colab_type": "text"
      },
      "source": [
        "* 인코더 훈련 : 언어에 상관없이 동일한 내용의 문장은 같은 값의 벡터로 인코딩한다.\n",
        "  * 판별자 : 인코딩된 문장의 언어를 맞춘다.\n",
        "  * 인코더 : 판별자를 속이도록 학습한다.\n",
        "  * 인코더의 출력값을 가지고 디코더를 통해 원래 문장으로 잘 돌아오도록 한다.\n",
        "  * 언어에 상관없이 1개씩의 인코더와 디코더 사용\n",
        "  * 다른 논문에서 제안한 사전훈련 모델 사용\n",
        "* 손실 함수 : 디노이징 오토인코더, 크로스 도메인 훈련, 적대적 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAZyvWK-xJr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "* 디노이징 오토인코더\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}