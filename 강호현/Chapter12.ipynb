{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter12.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOH81A9ENfAso3QVyFetPpk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CUAI-Deeplearning/Pytorch-NLP-Study/blob/stats/%EA%B0%95%ED%98%B8%ED%98%84/Chapter12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOu6-wVWtIRT",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 12. 강화학습을 활용한 자연어 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oIrre-MuY9L",
        "colab_type": "text"
      },
      "source": [
        "## 12.6 강화학습을 활용한 비지도학습\n",
        "\n",
        "* 지도학습 : 높은 정확도, 높은 비용과 시간(레이블 데이터 필요)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo1H6WnjtaRN",
        "colab_type": "text"
      },
      "source": [
        "* 비지도학습 : 데이터 확보 시 비용과 시간 훨씬 절감 가능, 효율은 떨어진다.\n",
        "* 병렬 코퍼스에 비해 쉬운 단일 언어 코퍼스가 대안이 된다.\n",
        "* 소량의 병렬 코퍼스 + 다량의 단일 언어 코퍼스 = 더 나은 성능 확보 가능\n",
        "* back translation, copied translation에도 NMT 성능 고도화 가능하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cxvBRy6uiyS",
        "colab_type": "text"
      },
      "source": [
        "### 12.6.1 비지도학습을 활용한 NMT\n",
        "* Cross-lingual Language Model Pretraining\n",
        "  * 오직 단일 언어 코퍼스만을 사용해 번역기 제작\n",
        "  * 언어에 상관 없이 같은 의미의 문장일 경우 인코더가 같은 값으로 임베딩하도록 훈련한다.\n",
        "  * 생성적 적대 신경망(GAN) 도입\n",
        "  * 인코더의 출력값이 연속적인 값이므로 GAN 적용 가능하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db7lOoCQwDlH",
        "colab_type": "text"
      },
      "source": [
        "* 인코더 훈련 : 언어에 상관없이 동일한 내용의 문장은 같은 값의 벡터로 인코딩한다.\n",
        "  * 판별자 : 인코딩된 문장의 언어를 맞춘다.\n",
        "  * 인코더 : 판별자를 속이도록 학습한다.\n",
        "  * 인코더의 출력값을 가지고 디코더를 통해 원래 문장으로 잘 돌아오도록 한다.\n",
        "  * 언어에 상관없이 1개씩의 인코더와 디코더 사용\n",
        "  * 다른 논문에서 제안한 사전훈련 모델 사용\n",
        "* 손실 함수 : 디노이징 오토인코더, 크로스 도메인 훈련, 적대적 학습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAZyvWK-xJr9",
        "colab_type": "text"
      },
      "source": [
        "* 디노이징 오토인코더\n",
        "  * 오토인코더에게 노이즈를 섞어준 소스 문장에서 노이즈 제거하면서 입력값을 출력에서 복원할 수 있도록 훈련한다.\n",
        "  * 디노이징 오토인코더(denoising autoencoder)\n",
        "  $$\n",
        "  \\mathcal{L}_{auto}(\\theta_{enc},\\theta_{dec}, \\mathcal{Z},\\ell) = \\mathbb{E}_{x \\sim D_{\\ell}, \\hat{x} \\sim d(e(C(x),\\ell),\\ell)}[\\vartriangle(\\hat{x},x)]$$\n",
        "  * $\\hat{x} \\sim d(e(C(x),\\ell),\\ell$ : 입력 문장 x를 노이즈 모델 C를 통해 노이즈 노이즈 더하고, 같은 언어 $\\ell$로 인코딩과 디코딩 수행\n",
        "  * $\\vartriangle(\\hat{x},x)$ : MRT에서와 같이 원문과 복원된 문장과의 차이\n",
        "  * 노이즈 모델 $C(x)$ : 임의로 문장 내 단어를 드롭하거나 순서 섞기(드롭 비율 0.1, 순서 섞는 윈도우 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9WReDfw0MN_",
        "colab_type": "text"
      },
      "source": [
        "* 크로스 도메인 훈련\n",
        "  * 사전 번역을 통해 사전 훈련한 저성능 번역모델 M에서 언어($\\ell_2$) 노이즈 추가되어 번역된 문장 y를 다시 언어($\\ell_1$) 소스 문장으로 원상 복구하는 작업을 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCF7s9hpPMcT",
        "colab_type": "text"
      },
      "source": [
        "  $$y = M(x) \\\\\n",
        "  \\mathcal{L}_{cd}(\\theta_{enc},\\theta_{dec}, \\mathcal{Z}, \\ell_1, \\ell_2) = \\mathbb{E}_{x \\sim D_{\\ell_1}, \\hat{x} \\sim d(e(C(y), \\ell_2), \\ell_1)}[\\vartriangle(\\hat{x},x)] $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCqM9xWN6DJf",
        "colab_type": "text"
      },
      "source": [
        "* 적대적 학습\n",
        "  * 인코더가 언어와 상관없이 항상 같은 분포로 잠재 공간에 문장 벡터를 임베딩하는지 감시하는 판별자 추가되어 적대적 학습을 진행한다.\n",
        "  * 판별자 : 잠재 변수 Z의 원래 언어를 예측하도록 훈련\n",
        "  * $x_i, \\ell_i$ : 같은 언어 쌍(language pair)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85qHL3BvPQmB",
        "colab_type": "text"
      },
      "source": [
        "  $$\\mathcal{L}_D(\\theta_D | \\theta, \\mathcal{Z}) = -\\mathbb{E}_{(x_i, \\ell_i)}[\\log p_D(\\ell_i | e(x_i, \\ell_i))] \\\\\n",
        "  \\mathcal{L}_{adv}(\\theta_{enc},\\mathcal{Z}|\\theta_D) = -\\mathbb{E}_{(x_i, \\ell_i)}[\\log p_D(\\ell_j | e(x_i, \\ell_i))] \\\\\n",
        "  \\text{where }j = -(i-1)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhqSQRve8OJP",
        "colab_type": "text"
      },
      "source": [
        "* 최종 목적 함수 : 3가지 목적 함수 결합하여 구한다.\n",
        "  * 각 $\\lambda$를 통해 손실 함수 상에서 비율 조절하여 최적의 파라미터 $\\theta$ 찾는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToBg8lwcPJIc",
        "colab_type": "text"
      },
      "source": [
        "  $$ \\mathcal{L}(\\theta_{enc},\\theta_{dec}, \\mathcal{Z}) = \\lambda_{auto}[\\mathcal{L}_{auto}(\\theta_{enc},\\theta_{dec},\\mathcal{Z}, \\ell_{src}) + \\mathcal{L}_{auto}(\\theta_{enc},\\theta_{dec},\\mathcal{Z}, \\ell_{tgt})]\\\\  +  \\lambda_{cd}[\\mathcal{L}_{cd}(\\theta_{enc},\\theta_{dec},\\mathcal{Z}, \\ell_{src}, \\ell_{tgt}) + \\mathcal{L}_{cd}(\\theta_{enc},\\theta_{dec},\\mathcal{Z}, \\ell_{tgt},\\ell_{src})]\\\\ + \\lambda_{adv} \\mathcal{L}_{adv}(\\theta_{enc}, \\mathcal{Z} | \\theta_D)\n",
        "  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEtaD0oD-T-y",
        "colab_type": "text"
      },
      "source": [
        "* 본 논문에서는 오직 단일 언어 코퍼스만 존재할 때 번역기 제작 방식만을 다룬다.\n",
        "  * 활용 가능성은 낮다.\n",
        "  * 단일 언어 코퍼스만으로는 성능이 낮다.\n",
        "  * 비용을 들여 병렬 코퍼스를 직접 구축하고, 병렬 코퍼스와 다수의 단일 언어 코퍼스 합쳐 번역기 구축할 것이기 때문이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXuSYPBD-o9N",
        "colab_type": "text"
      },
      "source": [
        "## 12.7 마치며\n",
        "* 강화학습 알고리즘을 사용해 자연어 생성 문제의 성능을 높인다.\n",
        "  * 폴리시 그래디언트로 자연어 생성에 적용\n",
        "\n",
        "* 폴리시 그래디언트 방법 이점\n",
        "  * 자기회귀 속성으로 실제 추론 방식과 다른 teacher-forcing 훈련 방법 탈피\n",
        "   * 실제 추론 방식과 같은 샘플링으로 문장 생성 능력 향상 가능\n",
        "  * 더 정확한 목적 함수 훈련 가능\n",
        "    * 기존의 PPL\n",
        "      * 번역 품질 혹은 문장 생성 품질 정확히 반영 불가하여 BLEU 혹은 다른 방법으로 모델 성능 측정 가능\n",
        "      * BLEU 미분 불가능하여 PPL의 교차 엔트로피로 신경망 훈련\n",
        "    * 폴리시 그래디언트는 보상 함수에 미분할 필요가 없어서 어떤 보상 함수든지 신경망 훈련 가능\n",
        "* 폴리시 그래디언트 단점\n",
        "  * 많은 반복 시행 필요(샘플링 기반)\n",
        "  * 훈련에 많은 시간 소요하여 비효율적인 학습 진행\n",
        "  * 보상함수 : 방향 없는 스칼라값 리턴하므로 보상함수 최대화하는 방향 정확히 알 수 없다.\n",
        "    * 기존의 MLE 방식 : 손실 함수를 신경망 파라미터 $\\theta$에 대해 미분하여 얻은 그래디언트를 통해 손실 함수 자체 최소화아여 업데이트 하는 방식\n",
        "    * 이보다 훨씬 비효율적인 학습 진행\n"
      ]
    }
  ]
}