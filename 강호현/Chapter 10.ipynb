{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Chapter 10.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CUAI-Deeplearning/Pytorch-NLP-Study/blob/stats/%EA%B0%95%ED%98%B8%ED%98%84/Chapter%2010.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3ePwTMHZ4pY",
        "colab_type": "text"
      },
      "source": [
        "# 10. 신경망 기계번역\n",
        "* 기계번역\n",
        "* seq2seq\n",
        "* 어텐션\n",
        "* input feeding\n",
        "* 자기회귀 속성과 teacher forcing 훈련 방법\n",
        "* 탐색(추론)\n",
        "* 성능 평가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_Wq81voZ4pc",
        "colab_type": "text"
      },
      "source": [
        "## 10.1. 기계번역\n",
        "\n",
        "* 기계번역(machine translation, MT) : 자연어 처리 영역에서의 종합예술\n",
        "* 규칙 기반 기계번역(RBMT)\n",
        "* 통계 기반 기계번역(SMT)\n",
        "* 신경망 기계번역(NMT) : 가장 큰 성취"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3ptlXYuZ4pe",
        "colab_type": "text"
      },
      "source": [
        "### 10.1.1 번역의 목표\n",
        "* 번역의 궁극적인 목표\n",
        "    * 언어 $f$의 문장이 주어질 때, 가능한 언어 $e$로의 번역 문장 중에서 최대 확률을 갖는 $\\hat{e}$ 찾아내기.\n",
        "    \n",
        "    $$\\hat{e} = argmax P_{f\\rightarrow e}(e|f) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vCb6bQoZ4pg",
        "colab_type": "text"
      },
      "source": [
        "* 번역이 어려운 이유\n",
        "    * 인간의 언어는 컴퓨터 프로그래밍 언어처럼 명확하지 않는다.\n",
        "    * 언어의 모호성 적극 활용하여 의사소통의 효율을 극대화한다.\n",
        "    * 언어는 민족의 문화를 담고 있어 문화마다 차이가 발생한다.\n",
        "    * 이 세가지는 기계가 우리의 말을 번역할 때 장벽이 된다.\n",
        "    \n",
        "* 왜 번역 기술은 중요할까?\n",
        "    * 기계번역을 통해 많은 일이 일어난다.\n",
        "    * 같은 세계인이 소통하는 SNS, 전 세계 대상의 인터넷 쇼핑몰 번역 서비스 제공\n",
        "    * 사용자 편리함 추구"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3hSt4QXZ4pl",
        "colab_type": "text"
      },
      "source": [
        "### 10.1.2 기계번역의 역사\n",
        "* 규칙 기반 기계번역(RBMT)\n",
        "    * 가장 전통적인 번역 방식\n",
        "    * 주어진 문장의 구조 분석 $\\rightarrow$ 규칙 세우고 분류 $\\rightarrow$ 정해진 규칙에 따라 번역 \n",
        "    * 사람과 달리 컴퓨터는 일반화 능력이 매우 떨어지므로 어렵다.\n",
        "    * 잘 만들어진 규칙에서는 SMT보다 자연스러운 표현 가능\n",
        "    * 규칙을 일일이 사람이 만드므로 번역기 제작에 많은 자원, 시간 소모\n",
        "    * 번역 언어쌍 확장시 매번 새로운 규칙을 찾고 적용해야 하므로 매우 불리\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UGK1NfWZ4pm",
        "colab_type": "text"
      },
      "source": [
        "* 통계 기반 기계번역(SMT)\n",
        "    * 신경망 기계번역 이전에 지배적인 번역 방식\n",
        "    * 대량의 양방향 코퍼스에서 통계량을 얻어서 번역 시스템 구성\n",
        "    * 구글 번역 시스템에 도입\n",
        "    * 여러 모듈로 구성하여 매우 복잡\n",
        "    * 언어쌍 확장시 대부분 알고리즘이나 시스템 유지되므로 기존의 RBMT보다 유리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqOHCHQ4Z4pn",
        "colab_type": "text"
      },
      "source": [
        "* 딥러닝 이전의 신경망 기계번역\n",
        "    * 1980년대 NMT 시도 존재\n",
        "    * 인코더 $\\rightarrow$ 디코더 형태의 구조\n",
        "    * 지금과 같은 성능 불가하여 외면\n",
        "    * 오토인코더 기반의 신경망 기계번역"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyRTSPcyZ4pn",
        "colab_type": "text"
      },
      "source": [
        "* 딥러닝 이후의 신경망 기계번역\n",
        "    * 현재의 딥러닝 방식(NMT)가 기존의 SMT 크게 앞지른다.\n",
        "    * 딥러닝 기반 번역기술로 거의 모든 상용 번역기 대체\n",
        "* 신경망 기계번역의 장점과 잘 동작하는 이유\n",
        "    * end-to-end 모델\n",
        "        * SMT는 번역 시스템이 여러가지 모듈로 구성되어 시스템 복잡도가 높아져 훈련하기 어렵다. NMT는 단 하나의 모델로 번역을 해결하여 성능 극대화|\n",
        "    * 더 나은 언어 모델\n",
        "        * 신경망 언어 모델(NNLM, Neural Network Language Model) 기반 구조이므로 SMT 기반 n-gram 방식의 언어 모델보다 더 강하다.\n",
        "        * 희소성 문제가 해결되고 자연스러운 번역 결과 문장의 생성에서 강점을 가진다.\n",
        "    * 훌륭한 문장 임베딩\n",
        "        * 신경망으로 문장을 차원 축소하여 임베딩 벡터로 제작하는 능력 우수\n",
        "        * 단어 단위보다 문장 단위에서 문제가 심각한 노이즈나 희소성 문제 잘 대처 가능\n",
        "        \n",
        "<img src = \"./asset/chap10/lecture10.png\" alt=\"Example\" width=\"500\" height=\"300\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIoTkX1fZ4pp",
        "colab_type": "text"
      },
      "source": [
        "## 10.2 seq2seq\n",
        "\n",
        "### 10.2.1 구조 소개\n",
        "* seq2seq 모델 구조를 활용하여 MLE 수행\n",
        "* 주어진 데이터를 가장 잘 설명하는 파라미터 $\\theta$ 찾기\n",
        "\n",
        "$$\\hat{\\theta} = \\underset{\\theta}{argmax}\\ P(Y|X;\\theta)\n",
        "\\text{ where }=\\{x_1, x_2, \\cdots, x_n\\}, Y=\\{y_1, y_2, \\cdots, y_m\\}$$\n",
        "\n",
        "$\\leftrightarrow P(Y|X;\\theta)$ 최대로 하는 모델 파라미터 찾는 작업"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_VaFe7nZ4pr",
        "colab_type": "text"
      },
      "source": [
        "* 사후 확률 최대로하는 $Y$ 찾기\n",
        "\n",
        "$$\\hat{Y} = \\underset{Y \\in \\mathcal{Y}}{argmax}\\ P(Y|X;\\theta)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-SfADLVZ4pt",
        "colab_type": "text"
      },
      "source": [
        "* seq2seq는 크게 3개 서브모듈인 인코더, 디코더, 생성자(generator) 구성\n",
        "* 인코더\n",
        "    * 주어진 소스 문장인 여러 벡터를 입력으로 받아 문장을 함축하는 문장 임베딩 벡터로 제작  \n",
        "    * $\\leftrightarrow P(z|X)$ 모델링\n",
        "    * RNN 분류 모델과 거의 동일하다.\n",
        "    * $P(z|X)$를 모델링 $\\rightarrow$ 주어진 문장을 매니폴드 따라 차원축소 $\\rightarrow$ 해당 도메인 잠재 공간의 하나의 점에 투영 작업\n",
        "    * 기존 텍스트 분류는 모든 정보(특징 feature)가 필요하지 않는다.\n",
        "    * 반면, 기계번역을 위한 문장 임베딩 벡터를 생성하기 위해 최대한 많은 정보 필요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxuXYUhfZ4pu",
        "colab_type": "text"
      },
      "source": [
        "* 인코더 수식\n",
        " $$h_t^{src} = RNN_{enc}(emb_{src}(x_t),h_{t-1}^{src}) \\\\\n",
        " H^{src} = [h_1^{src};h_2^{src};\\cdots;h_n^{src}]$$\n",
        " \n",
        "    * [;] : 이어 붙이는 작업(concatenate)\n",
        "    * time-step 별로 RNN 통과시켰다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6VvHxylZ4pu",
        "colab_type": "text"
      },
      "source": [
        "* 실제 구현\n",
        "    * 전체 time-step 병렬로 한번에 처리\n",
        "$$H^{src} = RNN_{enc}(emb_{src}(X),h_{0}^{src})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJzZARwKZ4pv",
        "colab_type": "text"
      },
      "source": [
        "* 디코더\n",
        "    * 신경망 언어 모델(NNLM) 연장선, 조건부 신경망 언어 모델(CNNLM)\n",
        "    * seq2seq 모델의 수식을 time-step에 관해 표현\n",
        "    $$P_{\\theta}(Y|X) = \\prod_{i=1}^m P_{\\theta}(y_t|X, y_{<t}) \\\\\n",
        "    \\log P_{\\theta}(Y|X) = \\sum_{i=1}^m \\log P_{\\theta}(y_t|X, y_{<t})$$\n",
        "    * RNNLM 조건부 확률 변수에 $X$ 추가\n",
        "    * 인코더 결과인 문장 임베딩 벡터와 이전 time-step까지 번역하여 생성한 단어 기반하여 현재 time-step 단어 생성\n",
        "    \n",
        "    $$h_t^{tgt} = RNN_{dec} (emb_{tgt}(y_{t-1}), h_{t-1}^{tgt}) \\\\\n",
        "    \\text{where } h_0^{tgt} = h_n^{src}\\text{ and }y_0=\\text{BOS}$$\n",
        "    * 디코더 수식으로, 디코더 또한 신경망 언어 모델\n",
        "    * 디코더 입력 초깃값으로 $y_0$에 BOS 토큰을 입력으로 함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEyiIo-5Z4pw",
        "colab_type": "text"
      },
      "source": [
        "* 생성자(generator)\n",
        "    * 디코더에서 각 time-step 별로 결과 벡터 h_t^{tgt} 받아 softmax 계산\n",
        "    * 각 타깃 언어의 단어별 확률값을 반환하는 단순 작업 수행 모듈\n",
        "    * 생성자의 결괏값 = 각 단어 나타난 확률, 이산 확률 분포가 된다.\n",
        "    * 주의할 점\n",
        "        * 문장 길이 $|Y| = m$일 때, 맨 마지막 반환되는 단어 $y_m$은 EOS 토큰이 된다.\n",
        "        * 마지막 $y_m$ = EOS, 디코더 계산의 종료이므로 디코더 입력으로 들어가지 않는다.\n",
        "        \n",
        "        $$ \\hat{y_t} = softmax(linear_{hs\\rightarrow |V_{tgt}|}(h_t^{tgt}))\\text{ and } \\hat{y_m}=EOS \\\\\n",
        "        \\text{ where }hs \\text{ is hidden size of RNN, and }|V_{tgt}|\\text{ is size of output vocabulary}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzQxckkIZ4px",
        "colab_type": "text"
      },
      "source": [
        "### 10.2.2 seq2seq 활용 분야\n",
        "* seq2seq 모델 : 기계번역 문제뿐만 아니라 다양한 분야 적용 가능\n",
        "    * 시계열 데이터 또는 시퀀스 데이터 입력 / 출력\n",
        "* seq2seq 응용사례\n",
        "\n",
        "| 활용 분야 | 입력과 출력 |\n",
        "|:--------|:--------|\n",
        "|기계 번역 | 특정 언어 문장 $\\rightarrow$ 다른 언어 문장|\n",
        "|챗봇 | 사용자 문장 $\\rightarrow$ 대답|\n",
        "|요약 | 긴 문장 $\\rightarrow$ 같은 언어 요약된 문장|\n",
        "|기타 자연어 처리 | 사용자 문장 $\\rightarrow$ 프로그래밍 코드|\n",
        "|음성 인식 | 사용자 음성 $\\rightarrow$ 해당 언어 문자열(문장)|\n",
        "|독순술 | 입술 움직임의 동영상 $\\rightarrow$ 해당 언어 문장|\n",
        "|이미지 캡셔닝|  변형된 seq2seq 사용한 이미지 $\\rightarrow$ 그림 설명 문장|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piutJBcJZ4py",
        "colab_type": "text"
      },
      "source": [
        "### 10.2.3 한계점\n",
        "* seq2seq는 오토인코더의 일종, 특히 시계열 / 시퀀스 데이터에 강점 있는 모델\n",
        "* 한계점\n",
        "    * 장기 기억력\n",
        "        * 신경망 모델은 차원 축소를 통해 데이터 압축하는 데 탁월한 성능을 가진다.\n",
        "        * seq2seq 통해도 정보 압축은 한계가 있다.\n",
        "        * 한계로 인해 문장이나 time-step이 길어질 수록 압축 성능이 떨어진다.\n",
        "        * LSTM이나 GRU로 RNN보다 높은 성능을 낼 수 있으나 한계가 있다.?\n",
        "        \n",
        "    * 구조 정보의 부재\n",
        "        * 현재 딥러닝 자연어 처리는 문장을 이해할 때 단순히 시퀀스 데이터로 다루는 경향이 있다.\n",
        "        * 신경망이 알아서 구조 파악 기대\n",
        "        * 다음 단계에는 구조 정보도 필요하다.        \n",
        "    * 챗봇 또는 QA봇\n",
        "        * 당연한 이야기\n",
        "        * 대화의 흐름에서 대답은 질문보다 새로운 지식이나 문맥정보가 추가된다.\n",
        "        * 기존 seq2seq 문제(번역이나 요약) 문제는 새롭게 추가된 문제가 없어 잘 해결한다.\n",
        "        * 대화는 좀 더 발전된 구조 필요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXjWKReqZ4py",
        "colab_type": "text"
      },
      "source": [
        "### 10.2.4 파이토치 예제 코드\n",
        "* 기계번역 seq2seq 파이토치로 구현한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQh9IWlzZ4pz",
        "colab_type": "text"
      },
      "source": [
        "* 인코더 클래스\n",
        "    * 인코더 RNN : 텍스트 분류기 코드와 매우 유사\n",
        "    * 양방향(Bi-directional) LSTM 사용 $\\rightarrow$ 선언시 bidirectional = TRUE\n",
        "    * 양방향 LSTM 은닉 상태\n",
        "        * 기존의 단방향(uni-directional) LSTM 은닉 상태보다 2배 증가 $\\rightarrow$ hidden_size/2로 RNN hidden_size 이용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px734oAaZ4pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#인코더 클래스\n",
        "class Encoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, word_vec_dim, hidden_size, n_layers=4, dropout_p=.2):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        # 'batch_first' parameter 값에 유의하라.\n",
        "        # 또한, hidden_size는 기존 것의 절반으로 양방향이기 때문이다.\n",
        "        self.rnn = nn.LSTM(word_vec_dim,\n",
        "                          int(hidden_size / 2),\n",
        "                          num_layers=n_layers,\n",
        "                          dropout=dropout_p,\n",
        "                          bidirectional=True,\n",
        "                          batch_first=True\n",
        "                          )\n",
        "        \n",
        "    def forward(self, emb):\n",
        "        # |emb| = (batch_size, length, word_vec_dim)\n",
        "        \n",
        "        if isinstance(emb, tuple):\n",
        "            x, lengths = emb\n",
        "            x = pack(x, lengths.tolist(), batch_first = True)\n",
        "            \n",
        "        else:\n",
        "            x = emb\n",
        "        \n",
        "        y, h = self.rnn(x)\n",
        "        # |y| = (batch_size, length, hidden_size)\n",
        "        # |h[0]| = (num_layers * 2, batch_size, hidden_size / 2)\n",
        "        \n",
        "        \n",
        "        if isinstance(emb, tuple):\n",
        "            y, _ = unpack(y, batch_first = True)\n",
        "            \n",
        "        \n",
        "        return y, h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL9G_YFtZ4p3",
        "colab_type": "text"
      },
      "source": [
        "* pack_padded_sequence 함수\n",
        "    * 기존의 샘플별 미니배치를 time-step별로 표현한다.\n",
        "    * PackedSequence로 표현된 time-step별 미니배치는 각 time-step별 샘플의 숫자를 추가적인 정보로 가진다.\n",
        "    * 미니배치 내 가장 긴 길이의 문장부터 차례로 정렬되어야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9DxqmU0Z4p4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pack_padded_sequence\n",
        "a = [torch.tensor([1,2,3]), torch.tensor([3,4])]\n",
        "b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-csgEngOZ4p5",
        "colab_type": "code",
        "outputId": "a755edde-fe78-430e-95b9-223ba87bcb18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [3, 4, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkEP748DZ4p9",
        "colab_type": "code",
        "outputId": "215b3c19-ca78-450f-da48-4451d7958ab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True, lengths=[3,2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([1, 3, 2, 4, 3]), batch_sizes=tensor([2, 2, 1]), sorted_indices=None, unsorted_indices=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsPkXHg-Ed5L",
        "colab_type": "text"
      },
      "source": [
        "* 디코더 클래스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYcYi26eEpW2",
        "colab_type": "text"
      },
      "source": [
        "* 생성자 클래스"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKiqgUCDFAdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, output_size):\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    self.output = nn.Linear(hidden_size, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    y = self.softmax(self.output(x))\n",
        "\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAxdZBeJFsPF",
        "colab_type": "text"
      },
      "source": [
        "* 전체 seq2seq 클래스\n",
        "생략"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L26b6L1ZF_AT",
        "colab_type": "text"
      },
      "source": [
        "* 손실 함수  \n",
        "  * seq2seq : 기본적으로 각 time-step별 가장 확률이 높은 단어를 선택하는 분류 문제 $\\rightarrow$ 교차 엔트로피를 손실 함수로 사용\n",
        "  * 조건부 언어 모델 $\\rightarrow$ 퍼플렉시티를 통해 번역 모델의 성능 표현 가능\n",
        "  * 교차 엔트로피와 매우 깊은 연관 가짐\n",
        "\n",
        "* 파이토치에서 손실 함수 준비\n",
        "  * 실제 구현시 softmax + 교차 엔트로피  $\\rightarrow$ logsoftmax + 로그 가능도(NLL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnaIc0hGGoXH",
        "colab_type": "text"
      },
      "source": [
        "    loss_weight = torch.ones(output_size)\n",
        "    loss_weight[data_loader.PAD] = 0\n",
        "\n",
        "    crit = nn.NLLLoss(weight=loss_weight,\n",
        "                      reduction='sum',)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUkBhsICG4Nx",
        "colab_type": "text"
      },
      "source": [
        "* softmax 함수 대신 logsoftmax 함수로 로그 확률을 구한다.\n",
        "* 나머지 수식 작업 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjaGg8lGHJPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_loss(self, y_hat, y, crit=None):\n",
        "  crit = self.crit if crit is None else crit\n",
        "  loss = crit(y_hat.contiguous().view(-1, y_hat.size(-1)),\n",
        "              y.contiguous().view(-1)\n",
        "              )\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmLLeFTTHcIJ",
        "colab_type": "text"
      },
      "source": [
        "## 10.3 어텐션\n",
        "\n",
        "### 10.3.1 어텐션 이해하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te_JifghOiqN",
        "colab_type": "text"
      },
      "source": [
        "* 어텐션은 쿼리와 비슷한 값을 가진 키를 찾아 그 값을 얻는 과정\n",
        "  * json 혹은 프로그래밍 사용하는 key-value 방식과 비교"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOIGuwkLOyI9",
        "colab_type": "text"
      },
      "source": [
        "### 10.3.2 key-value 함수\n",
        "\n",
        "* 키-밸류 또는 파이썬 딕셔너리 자료형"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uiqsoRYPPdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic = {'computer':9, 'dog':2,'cat':3}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_AlgHujPTIN",
        "colab_type": "text"
      },
      "source": [
        "* key, value 값 입력 후 key를 통해 value 값 접근 가능\n",
        "* 쿼리가 주어질 때 key 값에 따라 value 값에 접근 가능하다.\n",
        "* 함수로 나타내면 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpp63q-lPeDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def key_value_func(query):\n",
        "  weights = []\n",
        "\n",
        "  for key in dic.keys():\n",
        "    weights += [is_same(key, query)]\n",
        "\n",
        "  weight_sum = sum(weights)\n",
        "  for i, w in enumerate(weights):\n",
        "    weights[i] = weights[i] / weight_sum\n",
        "  answer = 0\n",
        "\n",
        "  for weight, value in zip(weights, dic.values()):\n",
        "    answer += weight * value\n",
        "\n",
        "  return answer\n",
        "\n",
        "\n",
        "def is_same(key, query):\n",
        "  if key == query:\n",
        "    return 1. \n",
        "  else:\n",
        "    return .0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42OHKlylQBvg",
        "colab_type": "text"
      },
      "source": [
        "* 코드 내부\n",
        "  * 순차적으로 dic 변수 내부의 key 값들과 query 값을 비교\n",
        "  * key가 같은 경우 weights 변수에 1.0을 더함\n",
        "  * key가 다른 경우 weights 변수에 .0을 더함\n",
        "  * weights를 weights_sum으로 나누어 그 합이 1이 되도록 만든다.\n",
        "  * 다시 dic 내부의 value값들과 weights값에 대해 서로 곱한 뒤에 더한다.\n",
        "  * weight가 1.0인 경우에만 value 값을 answer에 더한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po23adurQYSB",
        "colab_type": "text"
      },
      "source": [
        "### 10.3.3 연속적인 key-value 함수\n",
        "* is_same 함수 대신 다른 함수 쓰기\n",
        "* key와 query 사이 유사도를 반환하는 how_similar 가상함수 가정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVVvGETlQiDw",
        "colab_type": "text"
      },
      "source": [
        "    query = 'puppy'\n",
        "    how_similar('computer', query)\n",
        "    # 0.1\n",
        "    how_similar('dog', query)\n",
        "    # 0.9\n",
        "    how_similar('cat', query)\n",
        "    # 0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0skCQ5aCRTkt",
        "colab_type": "text"
      },
      "source": [
        "* key_value 함수 : 이 경우 puppy 단어 테스트 결과는 다음과 같이 실행이 된다.\n",
        "\n",
        "      query = 'puppy'\n",
        "      key_value_func(query)\n",
        "      2.823\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chraSQ01Rw91",
        "colab_type": "text"
      },
      "source": [
        "$$2.823 = {0.1 \\over (0.9 + 0.7 + 0.1)} \\times 9 + {0.9 \\over (0.9+0.7+0.1)} \\times 2 + {0.7 \\over (0.9+0.7+0.1)} \\times 3$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9-OWTUgSR4P",
        "colab_type": "text"
      },
      "source": [
        "* 2.823 값의 의미\n",
        "  * 강아지와 고양이 컴퓨터 유사도 비율에 따른 dic 값의 비율을 지닌다.\n",
        "  * is_same 함수 사용시 두 값이 같은지 if 문을 통해 검사 후 값을 할당했다.\n",
        "    * 0과 1의 불연속적인 값 출력 $\\rightarrow$ how_similar 함수를 통해 0에서 1사이의 연속적인 값을 weights 할당 $\\rightarrow$ key_value_func 함수 수행\n",
        "  * key_value_func 딥러닝 사용 가능"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2-GDl-VSpbx",
        "colab_type": "text"
      },
      "source": [
        "### 10.3.4 연속적인 key-value 함수\n",
        "\n",
        "* dic의 value : 100차원 벡터 구성\n",
        "* 쿼리의 key 값 모두 벡터 다룬다.\n",
        "* 단어 임베딩 벡터이다.\n",
        "* how_similar 함수 : 이 벡터들 간의 코사인 유사도 반환한다.\n",
        "* dic의 key 값과 value 값이 서로 같다.\n",
        "\n",
        "* word2vec 함수 : 단어를 입력으로 받아 그 단어에 해당하는 미리 정해진 단어 임베딩 벡터를 반환 가정\n",
        "  * how_similar 함수 : 두 벡터 간의 코사인 유사도 값 반환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qyC-tZXS7Ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def key_value_func(query):\n",
        "  weights = []\n",
        "\n",
        "  for key in dic.keys():\n",
        "    weights += [how_similar(key, query)]  # 코사인 유사도 반환\n",
        "\n",
        "  weights = softmax(weights) \n",
        "  answer = 0\n",
        "\n",
        "\n",
        "  for weight, value in zip(weights, dic.values()):\n",
        "    answer += weight * value\n",
        "\n",
        "  return answer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcu8OTR-VRAY",
        "colab_type": "text"
      },
      "source": [
        "* key_value_func : 값을 받아 weight에 저장 $\\rightarrow$ 모든 weights 값이 채워지면 softmax 함수 취하기 \n",
        "  * softmax : weights 합의 크기를 1로 고정시키는 정규화 역할\n",
        "  * 유사도 총합에서 차지하는 비율만큼 weight의 값이 채워진다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUNKfPrSVx5K",
        "colab_type": "text"
      },
      "source": [
        "    from gensim.models.word2vec import Word2Vec\n",
        "    100\n",
        "    import gensim.models.word2vec as word2vec\n",
        "    [0.1, 0.3, -0.7, 0.0 ...]\n",
        "    len(word2vec('computer'))\n",
        "    [0.15, 0.2, -0.3, 0.0, ...]\n",
        "\n",
        "    dic = {word2vec('computer'): word2vec('computer),\n",
        "            ...}\n",
        "\n",
        "    query = 'puppy'\n",
        "    answer = key_Value_func(word2vec(query))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foxvnmG2aEpe",
        "colab_type": "text"
      },
      "source": [
        "* answer 값에 벡터 값이 들어 간다.\n",
        "  * puppy 벡터, dog, computer, cat 벡터들의 코사인 유사도에 따라 값이 정해진다.\n",
        "  * 이 함수는 query와 비슷한 key값을 찾아 비슷한 정도에 따라 weight 정한다.\n",
        "  * 각 key의 value 값을 weight 값만큼 가져와서 모두 더한다.  \n",
        "$\\therefore$어텐션의 원리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qw6K20ma7UU",
        "colab_type": "text"
      },
      "source": [
        "### 10.3.5 기계번역에서의 어텐션\n",
        "* 번역 과정에서 각 time-step별 출력을 키와 밸류로 삼는다.\n",
        "* 현재 time-step의 디코더 출력은 쿼리로 삼아 어텐션 계산  \n",
        "\n",
        "* RNN 기반 seq2seq에서 어텐션이 활용될 때 각 입력값\n",
        "\n",
        "|항목 |구성 |\n",
        "|--|--|\n",
        "|query |현재 time-step의 디코더 출력 |\n",
        "|keys |각 time-step별 인코더 출력 |\n",
        "|values |각 time-step별 인코더 출력 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf2woCeIbi9F",
        "colab_type": "text"
      },
      "source": [
        "    context_vector = attention(query = decoder_output,\n",
        "                           keys = encoder_outputs,\n",
        "                           values = encoder_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXzCby_Kbw3l",
        "colab_type": "text"
      },
      "source": [
        "* 어텐션 추가한 seq2seq 수식\n",
        "\n",
        "$$ w = softmax(h_t^{tgtT} W \\cdot H^{src}) \\\\\n",
        "c = H^{src} \\cdot \\text{ and $c$ is a context vector} \\\\\n",
        "\\tilde{h_t^{tgt}} = \\tanh(linear_{2\\times hs $rightarrow hs}([h_t^{tgt};c])) \\\\\n",
        "\\hat{y_t} = softmax(linear_{hs \\rightarrow |V_tgt|}(\\tilde{h_t^{tgt}}))\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWQgUQr5dCc8",
        "colab_type": "text"
      },
      "source": [
        "* 원하는 정보를 어텐션을 통해 인코더에서 획득 $\\rightarrow$ 해당 정보를 디코더의 출력과 이어붙여 tanh 취한다.\n",
        "* softmax 계산으로 다음 time-step의 입력이 되는 $\\hat{y_t}$ 구한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y9kdokOdRDW",
        "colab_type": "text"
      },
      "source": [
        "* 선형 변환\n",
        "  * 각 입력 파라미터 추상적으로 예상\n",
        "\n",
        "* 기계번역에서 인코더와 디코더 출력값 역할과 의미\n",
        "\n",
        "|항목 |의미|\n",
        "|--|--|\n",
        "|decoder_output |현재 time-step까지 번역된 타깃 언어의 단어 또는 문장, 의미 |\n",
        "|encoder_output |각 time-step 에서의 소스 언어의 단어 또는 문장, 의미 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9aMfW1CdodH",
        "colab_type": "text"
      },
      "source": [
        "* 신경망 내부의 각 차원 : 숨겨진 특징값(latent feature)이므로 정의 불가\n",
        "  * 소스 언어와 대상 언어가 애초에 다르다.\n",
        "  * 단순히 벡터 내적보다 연결고리 추가(소스, 대상언어)\n",
        "  * 두 언어가 각각 임베딩된 잠재 공간이 선형 관계 있다고 추상적으로 가정\n",
        "  * 내적 연산을 수행하기 전에 선형 변환을 한다.\n",
        "  * 이 선형 변환을 위한 $W$값은 신경망 가중치 파라미터, 피드포워드 및 역전파로 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db6DG4omeRYI",
        "colab_type": "text"
      },
      "source": [
        "* 어텐션의 필요성\n",
        "  * seq2seq : 인코더와 디코더 두 개의 RNN 구성\n",
        "  * 문장 임베딩 벡터 해당하는 인코더 결과 벡터 정보를 디코더 은닉 상태(LSTM은 cell state)가 추가로 전달\n",
        "  * 디코더 : 인코더에서 넘겨받은 은닉 상태로부터 문장 생성\n",
        "  * 은닉 상태만으로는 문장의 모든 정보를 완벽하게 전달이 어렵다.\n",
        "    * 문장이 길어질수록 문제가 크다.\n",
        "  * 디코더의 time-step마다 현재 디코더의 은닉 상태에 따라 필요한 인코더 정보(인코더 마지막 레이어 은닉 상태) 접근하여 끌어다 쓴다.\n",
        "\n",
        "* 선형 변환자체가 어텐션이다.\n",
        "  * 선형 변환 과정으로 디코더 현재 상태에 따라 필요한 쿼리 제작\n",
        "  * 인코더 key 값들과 비교하여 가중합을 한다.\n",
        "  * 어텐션을 통해 디코더는 인코더에 쿼리를 날린다.\n",
        "    * 쿼리 잘 날려야 좋은 정답을 얻는다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXFVvZyqf5fj",
        "colab_type": "text"
      },
      "source": [
        "* 신경망은 쿼리를 만들어내는 훈련을 하는 것이다.\n",
        "  * 현재 디코더의 상태에 따라 필요한 정보가 무엇인지를 스스로 판단\n",
        "  * 선형 변환을 통해 쿼리를 제작할 것이다.\n",
        "  * 선형 변환을 위한 가중치 파라미터 자체도 한계가 있다.\n",
        "  * 디코더 상태 자체가 선형 변환이 되어 쿼리가 좋은 형태가 되도록 RNN이 동작할 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56qO-f_Zh4DQ",
        "colab_type": "text"
      },
      "source": [
        "* 어텐션 적용 결과\n",
        "  * 어텐션 없는 seq2seq 성능이 떨어진다.\n",
        "  * 문장이 길어질수록 성능이 더욱 하락한다.\n",
        "  * 어텐션 사용시 문장이 길어져도 성능이 크게 하락하지 않는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdD8q6eViCBG",
        "colab_type": "text"
      },
      "source": [
        "### 10.3.6 파이토치 예제코드\n",
        "\n",
        "* torch.bmm 함수 : 배치 행렬 곱(BMM) 수행하는 함수\n",
        "  * 2개 이상의 차원을 지닌 텐서가 주어졌을 때 뒤의 2개 차원에 대해 행렬 곱을 수행\n",
        "  * 앞의 다른 차원은 미니배치로 취급한다.\n",
        "  * 앞의 차원들은 크기가 같아야 하고, 뒤의 2개 차원은 행렬 곱을 수행하기 위한 적절한 크기 필요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb37zfyiihiW",
        "colab_type": "text"
      },
      "source": [
        "* 파이토치 배치 행렬 곱 연산\n",
        "\n",
        "$$z = torch.bmm(x,y) \\\\\n",
        "(batch_size, m, k) \\times (batch_size, h. m) = (batch_size, n, m) \\\\\n",
        "\\Leftrightarrow x \\times y = z$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_HLoX9ti4YV",
        "colab_type": "text"
      },
      "source": [
        "    import torch\n",
        "    z = torch.bmm(x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P37pe4MbjU_7",
        "colab_type": "text"
      },
      "source": [
        "* 어텐션 클래스\n",
        "  * 선형 변환을 위한 가중치 파라미터를 편향(bias)없는 선형 계층으로 대체"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYvNPdE0jkDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size):\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "\n",
        "  def forward(self, h_src, h_t_tgt, mask=None):\n",
        "    query = self.linear(h_t_tgt.squeeze(1)).unsqueeze(-1)\n",
        "\n",
        "    weight = torch.bmm(h_src, query).squeeze(-1)\n",
        "\n",
        "    if mask is not None:\n",
        "\n",
        "      weight.masked_fill_(mask, -float('inf'))\n",
        "    weight = self.softmax(weight)\n",
        "\n",
        "    context_vector = torch.bmm(weight.unsqueeze(1), h_src)\n",
        "\n",
        "    return context_vector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEH2YRRMk0kD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}